{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fake-news-detector.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Design Interview of a Fake News Detector - Notes\n",
        "\n",
        "By Robin Forsberg (2022)"
      ],
      "metadata": {
        "id": "viY-I6th_Px4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The following notes simulate a client commission of designing a fake news detector system.*"
      ],
      "metadata": {
        "id": "2EjCMAcgE-s9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Client Expectations"
      ],
      "metadata": {
        "id": "Nm15tcsgA-nH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape news articles and fake news datasets from the web\n",
        "\n",
        "-Needs to support: HTTPS, Javascript, Login prompts, paywalls, CAPTCHAs\n",
        "\n",
        "-ScrapeBox, scrape stack, Scrapy\n",
        "\n",
        "-Download Fake News datasets from their source\n",
        "\n",
        "-Information Security and Object Tech (ISOT)\n",
        "\n",
        " \n",
        "\n",
        "Wrangle the data into a consistent structure for creating features.\n",
        "\n",
        "-Date and time format manipulations\n",
        "\n",
        "-Changing data types to match consistent schema\n",
        "\n",
        "-Replacing missing data with NAs\n",
        "\n",
        "-Joining data\n",
        "\n",
        "-Exporting processed data to storage layer \n",
        "\n",
        "-These functionalities can be designed from scratch using pyspark library. \n",
        "\n",
        "-Storage layer is postgresql. We will have table there where each row is keyed on a unique article id. The rows will contain columns including article contents themselves and article metadata such as the publication date, the publisher, article length and perhaps the news article author. The storage will be portioned based on date, so we can partition one for training the data. These articles will then be needed to be labeled as either ‘reliable’ or ‘unreliable’. If the data came from a fake news dataset, we won’t need to label them because they will be labeled for us.\n",
        "\n"
      ],
      "metadata": {
        "id": "hWTzW5o_AKLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. System specifications"
      ],
      "metadata": {
        "id": "ozbATm4dAHh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-ML solution to Social Media Platforms. \n",
        "\n",
        "-How is fake news defined?: ‘Let’s define fake news as news articles that contain intentionally misleading info from what appears to be a reliable source.'\n",
        "\n",
        "-Why are our customers: paying us?’: We want to limit the negative financial, health, and social impacts of misinformation on the platforms.’\n",
        "\n",
        "-Service is API based - the clients, the social media companies, will call our API solution.\n",
        "\n",
        "-The platforms call our service as users post news articles and depending on our services’s response:\n",
        "\n",
        "Allow the post\n",
        "\n",
        "*   Allow the post\n",
        "*   Limit exposure to the post\n",
        "*   Disallow the post"
      ],
      "metadata": {
        "id": "TNhZAPXu_ZPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Design"
      ],
      "metadata": {
        "id": "Q6b3tUr4BQ3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've scraped the data, figure out how to label the newly scraped news articles. Labelling must here be done manually. \n",
        "\n",
        "Use an internal workforce or\n",
        "3rd party Service such as Mechanical Turk. Then:\n",
        "-Establish comprehensive labelling guidelines\n",
        "-Extract linguistic features from the articles\n",
        "\n",
        "Linguistic Inquire and Word Count (LIWC) (this is our primary source). This process will extract over 90 features for us, eg.:\n",
        "Summary Language Variables (words per sentence, the tone of the article, words with more than 6 letters, different counts like that)\n",
        "Linguistic Dimensions (total pronouns)\n",
        "Psychological Processes (positive or negative emotions, social references)"
      ],
      "metadata": {
        "id": "4095uzzmBp1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then proceed to modelling. In this case, we'll train a random forest algorithm with the following specifications:\n",
        "\n",
        "128 trees of max depth 6 \n",
        "200.000 total news articles (3mb of memory)\n",
        "\n",
        "At this stage it's good consider that there’s typically far more reliable articles than unreliable articles. Therefore our your model be able to handle this imbalance?\n",
        "\n",
        "The answer's no, that’s why need to implement class weighting. Class weighting can be used when mitigating the imbalanced data. All we have to do is to add a stronger weight for the minority class when calculating the gini-impurity for a particular split. Class weighting typically results in a higher rate of false positives so let’s look at another technique called SYNTHETIC MINORITY OVERSAMPLING TECHNIQUE. \n",
        "\n",
        "The core steps here are:\n",
        "\n",
        "1. Tune the model\n",
        "2. Calculate Max tree depth\n",
        "3. Calculate Number of trees\n",
        "4. Max number of leaf nodes\n",
        "5. Minimum impurity decrease\n",
        "6. Minimum samples in a leaf\n",
        "7. Minimum samples required to split\n",
        "8. Max features to evaluate\n",
        "9. FI-score will be a better metric than accuracy. Also Cohen’s Kappa."
      ],
      "metadata": {
        "id": "CG7VIInRCEnb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Initial model evalutions"
      ],
      "metadata": {
        "id": "HwOHaxqiDiXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I. At this stage, we have completed the model. Now we're faced with a question by the client: ‘Let’s say your model achieves a 95% accuracy. If your model predicts an article to be unreliable, what’s the probability that it actually is?’\n",
        "\n",
        "To calculate this, we'll use Bayesian inference:\n",
        "\n",
        "P(unreliable. | model+) = P(unreliable) x P(model+ |  unreliable) / P(Model+) <— Bayesian probability = .02 x .92 / (.02 x .92) + (.98 x .03) = 38% —> “The probability of an article actually being  unreliable is 38% given that our model says it is unreliable.”"
      ],
      "metadata": {
        "id": "6WQh4rhCDFuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "II. Second question to consider: ‘Okay, so is this any accurate / good? Does it actually help us?’ \n",
        "\n",
        "In this case, it actually is good, given that the average human accuracy of detecting a fake news article lies between 55 and 60 percent. If we hold the same proportion of false positive rates and true positive rates with that new assumption of 55 percent accuracy, then an average human has an 18 percent probability of an article being unreliable given that they think it is unreliable. Our model is roughly twice as good as that. "
      ],
      "metadata": {
        "id": "Le1vQbC5Dvya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. System architecture and deployment"
      ],
      "metadata": {
        "id": "bFOx3NgiELF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Collect articles via web scraper.\n",
        "2.   Feed articles to the data wrangler.\n",
        "3.   After processing the data, the data wrangler will send the data to the PosgreSQL-table. This database table will be keyed on a unique article ID and the columns will contain the article contents themselves as well as metadata of the articles.\n",
        "4. Next we'll send this to a labelling queue, and the labelling queue will be pulled by a labelling interface such that the labellers can be logged in to the labelling interface and start manually label these news articles as either reliable or unreliable. For the labelling queue, we can use AWS sq or Queue service, and for the labelling interface we can use a simple webapp hosted on a lightweight web app such as Flask. The Flask server can directly add labels to the label column in the same exact postgreSQL table used by the data wrangler for each article.\n",
        "5. Now when articles have been added to the database and consumed by the labelling interface, we can now extract features from these articles. We are going to have a feature extractor. The Feature Extractor is going to be activated the same way the Labelling Queue is activated by using the change data caption in the database. The difference is that we will be using Rabbit MQ as the broker, or queue here, and the Feature Extractor will be a cluster of Celery workers which will pull tasks from Rabbit MQ and perform the LIWC Feature Extracting. Since the lexicon of the LIWC FE is proprietary, we will need to integrate with their the RECEPTIVITY-API which holds the exclusive rights to LIWC-lexicons or work with them to get a custom solution to get a local local LIWC library so we don’t need to make an API-call for each article. In our case we are going to use FEAST.\n",
        "6. Once we have the summary data, we send it to ISOT (the fake news dataset), where we commence the model development.\n",
        "7. We split the data in test or train\n",
        "8. Conduct model training\n",
        "9. Model tuning\n",
        "10. Evaluation\n",
        "11. Deployment to the cloud (AWS / Heroku / other cloud provider).\n",
        "12. Create an interface for technical account manager to query the data easily. Elasticsearch helps to search log stash. Kibana and PageDuty function as the interface."
      ],
      "metadata": {
        "id": "4C-4hesoE1dx"
      }
    }
  ]
}